{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"i8Yljq8L6cHx","colab_type":"code","colab":{}},"cell_type":"code","source":["# Import libraries\n","from __future__ import print_function\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torch.nn.functional import sigmoid, relu\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","import os\n","import time"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Orftj5KD61LG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"6822e804-7ae7-4edd-9ed2-06e8bc0d5bd4","executionInfo":{"status":"ok","timestamp":1556571977058,"user_tz":240,"elapsed":368,"user":{"displayName":"Arihant Jain","photoUrl":"https://lh5.googleusercontent.com/-3kii2uSyBNA/AAAAAAAAAAI/AAAAAAAAHwU/T3ZvQABdreo/s64/photo.jpg","userId":"03591714780569092811"}}},"cell_type":"code","source":["# we will verify that GPU is enabled for this notebook\n","# following should print: CUDA is available!  Training on GPU ...\n","# \n","# if it prints otherwise, then you need to enable GPU: \n","# from Menu > Runtime > Change Runtime Type > Hardware Accelerator > GPU\n","\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"metadata":{"id":"sRdPOYyW63Nr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":290},"outputId":"8e6db160-b63b-4acd-851f-20a259099568","executionInfo":{"status":"ok","timestamp":1556571987204,"user_tz":240,"elapsed":338,"user":{"displayName":"Arihant Jain","photoUrl":"https://lh5.googleusercontent.com/-3kii2uSyBNA/AAAAAAAAAAI/AAAAAAAAHwU/T3ZvQABdreo/s64/photo.jpg","userId":"03591714780569092811"}}},"cell_type":"code","source":["with open('gdrive/My Drive/Colab Notebooks/items.txt','r') as fid:\n","    names_items = np.array([l.strip() for l in fid.readlines()])\n","with open('gdrive/My Drive/Colab Notebooks/relations.txt','r') as fid:\n","    names_relations = np.array([l.strip() for l in fid.readlines()])\n","with open('gdrive/My Drive/Colab Notebooks/attributes.txt','r') as fid:\n","    names_attributes = np.array([l.strip() for l in fid.readlines()])\n","        \n","nobj = len(names_items)\n","nrel = len(names_relations)\n","nattributes = len(names_attributes)\n","print('List of items:')\n","print(names_items)\n","print(\"List of relations:\")\n","print(names_relations)\n","print(\"List of attributes:\")\n","print(names_attributes)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["List of items:\n","['Beaver' 'Otter' 'Dolphin' 'Trout' 'Flatfish' 'Shark' 'Bear' 'Wolf'\n"," 'Tiger' 'Fox' 'Rabbit' 'Crocodile' 'Snake' 'Bee' 'Butterfly' 'Maple_tree'\n"," 'Oak_tree' 'Palm_tree' 'Pine_tree' 'Willow_tree' 'Orchid' 'Poppy' 'Tulip'\n"," 'Sunflower' 'Rose']\n","List of relations:\n","['ISA' 'Is' 'Can' 'Has']\n","List of attributes:\n","['Plant' 'Animal' 'Tree' 'Flower' 'Aquatic_animal' 'Land_animal' 'Mammal'\n"," 'Beaver' 'Otter' 'Dolphin' 'Trout' 'Flatfish' 'Shark' 'Bear' 'Wolf'\n"," 'Tiger' 'Fox' 'Rabbit' 'Crocodile' 'Snake' 'Bee' 'Butterfly' 'Maple_tree'\n"," 'Oak_tree' 'Palm_tree' 'Pine_tree' 'Willow_tree' 'Orchid' 'Poppy' 'Tulip'\n"," 'Sunflower' 'Rose' 'Pretty' 'Big' 'Living' 'Green' 'Red' 'Yellow' 'White'\n"," 'Grow' 'Move' 'Swim' 'Fly' 'Walk' 'Growl' 'Leaves' 'Roots' 'Skin' 'Legs'\n"," 'Bark' 'Branches' 'Petals' 'Wings' 'Feathers' 'Scales' 'Gills' 'Fur']\n"],"name":"stdout"}]},{"metadata":{"id":"FqMcQZ6r65r-","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","infile = open('gdrive/My Drive/Colab Notebooks/Final_dataset','rb')\n","new_dict = pickle.load(infile)\n","infile.close()\n","infile = open('gdrive/My Drive/Colab Notebooks/Final_dataset_test','rb')\n","test_dict = pickle.load(infile)\n","infile.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d7FlkWH568JA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":324},"outputId":"5efdbca2-d8d5-457e-ef75-c1567d09fe5f","executionInfo":{"status":"ok","timestamp":1556572011373,"user_tz":240,"elapsed":514,"user":{"displayName":"Arihant Jain","photoUrl":"https://lh5.googleusercontent.com/-3kii2uSyBNA/AAAAAAAAAAI/AAAAAAAAHwU/T3ZvQABdreo/s64/photo.jpg","userId":"03591714780569092811"}}},"cell_type":"code","source":["new_dict.keys\n","print(new_dict['data'].shape)\n","image1 = (new_dict['data'][1,:].reshape(3,32,32).transpose(1,2,0))\n","plt.imshow(image1)\n","print(image1.shape)\n","image2 = (test_dict['data'][0,:].reshape(3,32,32).transpose(1,2,0))\n","plt.imshow(image2)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(50000, 3072)\n","(32, 32, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f2074d8b6d8>"]},"metadata":{"tags":[]},"execution_count":6},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"Ma7Dwt6X6_i-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"e409d654-13ec-47fb-e46d-b88fc48086c6","executionInfo":{"status":"ok","timestamp":1556572018133,"user_tz":240,"elapsed":322,"user":{"displayName":"Arihant Jain","photoUrl":"https://lh5.googleusercontent.com/-3kii2uSyBNA/AAAAAAAAAAI/AAAAAAAAHwU/T3ZvQABdreo/s64/photo.jpg","userId":"03591714780569092811"}}},"cell_type":"code","source":["train_images = new_dict['data'].reshape((len(new_dict['data']), 3, 32, 32))\n","print(train_images.shape)\n","test_images = test_dict['data'].reshape((len(test_dict['data']), 3, 32, 32))\n","print(test_images.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(50000, 3, 32, 32)\n","(10000, 3, 32, 32)\n"],"name":"stdout"}]},{"metadata":{"id":"pKl9-uKG7BPH","colab_type":"code","colab":{}},"cell_type":"code","source":["#means = np.mean(train_images,axis=(0,1,2))/255\n","#stds = np.std(train_images, axis=(0,1,2))/255\n","#print(means)\n","#print(stds)\n","train_images = train_images/255\n","test_images = test_images/255"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p55sAYV97Dbn","colab_type":"code","colab":{}},"cell_type":"code","source":["from torchvision import transforms, utils\n","train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_images,dtype=torch.float),torch.tensor(new_dict['label'],dtype=torch.float))\n","trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=300, shuffle=True, num_workers=2)\n","# get some random training images\n","#dataiter = iter(trainloader)\n","#images, labels = dataiter.next()\n","#labels.shape\n","test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_images,dtype=torch.float),torch.tensor(test_dict['label'],dtype=torch.float))\n","testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n","#dataiter_test = iter(testloader)\n","#images_test, labels_test = dataiter_test.next()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-35RgwHK7FSb","colab_type":"code","colab":{}},"cell_type":"code","source":["'''AlexNet for CIFAR10. FC layers are removed. Paddings are adjusted.\n","Without BN, the start learning rate should be 0.01\n","(c) YANG, Wei \n","'''\n","import torch.nn as nn\n","\n","class AlexNet(nn.Module):\n","\n","    def __init__(self, num_hidden):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.fc_hidden = nn.Linear(256 + 4, num_hidden)\n","        self.fc_output = nn.Linear(num_hidden, 57)\n","\n","    def forward(self, x, y):\n","        x = self.features(x)\n","        rep = x.view(x.size(0), -1)\n","        hidden = relu(self.fc_hidden(torch.cat((rep,y),1)))\n","        output = sigmoid(self.fc_output(hidden))\n","        return output, hidden, rep\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pKi45d_z7H5d","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(mynet,epoch_count,nepochs_additional=100):\n","    # Input\n","    #  mynet : Net class object\n","    #  epoch_count : (scalar) how many epochs have been completed so far\n","    #  nepochs_additional : (scalar) how many more epochs we want to run\n","    mynet.train()\n","    for e in range(nepochs_additional): # for each epoch      \n","      error_epoch = 0.\n","      for batch_idx, (data,target) in enumerate(trainloader):\n","        t0 = time.time()\n","        data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n","        data, target = data.cuda(), target.cuda()\n","        outputs, hidden, rep = mynet(data, target[:,:4])\n","        loss = criterion(outputs, target[:,4:])\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        error_epoch += loss.item()\n","      print('epoch ' + str(epoch_count+e) + ' loss ' + str(round(error_epoch,3)))\n","      print('time taken for the epoch ' + str(round((time.time()-t0),3)))\n","    return epoch_count + nepochs_additional"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PQmT07JG7Jz-","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_rep(net):\n","    # Extract the hidden activations on the Representation Layer for each item\n","    # \n","    # Input\n","    #  net : Net class object\n","    #\n","    # Output\n","    #  rep : [nitem x rep_size numpy array], where each row is an item\n","    rep_avg = np.zeros([25,256])\n","    for idx,(data, target) in enumerate(testloader):\n","      if idx == 25:\n","        break\n","      data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n","      data, target = data.cuda(), target.cuda()\n","      outputs, hidden, rep = mynet(data, target[:,:4])\n","      rep_avg[idx,:] = np.mean(rep.cpu().detach().numpy(),axis=0,keepdims=True)\n","    print(rep_avg.shape)\n","    return rep_avg\n","\n","def plot_dendo(rep1,rep2,rep3,names):\n","    #  Compares Representation Layer activations of Items at three different times points in learning (rep1, rep2, rep3)\n","    #  using hierarchical clustering\n","    # \n","    #  Each rep1, rep2, rep3 is a [nitem x rep_size numpy array]\n","    #  names : [nitem list] of item names\n","    #\n","    nepochs_list = [nepochs_phase1,nepochs_phase2,nepochs_phase3]\n","    linked1 = linkage(rep1,'single')\n","    linked2 = linkage(rep2,'single')\n","    linked3 = linkage(rep3,'single')\n","    mx = np.dstack((linked1[:,2],linked2[:,2],linked3[:,2])).max()+0.1    \n","    plt.figure(2,figsize=(7,12))\n","    plt.subplot(3,1,1)    \n","    dendrogram(linked1, labels=names, color_threshold=0)\n","    plt.ylim([0,mx])\n","    plt.title('Hierarchical clustering; ' + \"epoch \" + str(nepochs_list[0]))\n","    plt.ylabel('Euclidean distance')\n","    plt.subplot(3,1,2)\n","    plt.title(\"epoch \" + str(nepochs_list[1]))\n","    dendrogram(linked2, labels=names, color_threshold=0)\n","    plt.ylim([0,mx])\n","    plt.subplot(3,1,3)\n","    plt.title(\"epoch \" + str(nepochs_list[2]))\n","    dendrogram(linked3, labels=names, color_threshold=0)\n","    plt.ylim([0,mx])\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2k1-8bQC7Ma9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":18466},"outputId":"37b1866b-c0ca-46b6-9e27-1d23106be0ac","executionInfo":{"status":"error","timestamp":1556576671607,"user_tz":240,"elapsed":2599936,"user":{"displayName":"Arihant Jain","photoUrl":"https://lh5.googleusercontent.com/-3kii2uSyBNA/AAAAAAAAAAI/AAAAAAAAHwU/T3ZvQABdreo/s64/photo.jpg","userId":"03591714780569092811"}}},"cell_type":"code","source":["def train(mynet,epoch_count,nepochs_additional=100):\n","    # Input\n","    #  mynet : Net class object\n","    #  epoch_count : (scalar) how many epochs have been completed so far\n","    #  nepochs_additional : (scalar) how many more epochs we want to run\n","    dolphincan = np.zeros((6,nepochs_additional))\n","    animals = np.zeros((15,nepochs_additional,256))\n","    plants = np.zeros((10,nepochs_additional,256))\n","    fish = np.zeros((2,nepochs_additional,256))\n","    fish2 = np.zeros((2,nepochs_additional,256))\n","    for e in range(nepochs_additional): # for each epoch      \n","      error_epoch = 0.\n","      mynet.train()\n","      for batch_idx, (data,target) in enumerate(trainloader):\n","        t0 = time.time()\n","        data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n","        data, target = data.cuda(), target.cuda()\n","        outputs, hidden, rep = mynet(data, target[:,:4])\n","        loss = criterion(outputs, target[:,4:])\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        error_epoch += loss.item()\n","      mynet.eval()  \n","      for idx,(data, target) in enumerate(testloader):\n","        if idx == 52:\n","          data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n","          data, target = data.cuda(), target.cuda()\n","          outputs, hidden, rep = mynet(data, target[:,:4])\n","          dolphincan[0,e] = np.mean(outputs.cpu().detach().numpy(),axis=0,keepdims=True)[0,39]\n","          dolphincan[1,e] = np.mean(outputs.cpu().detach().numpy(),axis=0,keepdims=True)[0,40]\n","          dolphincan[2,e] = np.mean(outputs.cpu().detach().numpy(),axis=0,keepdims=True)[0,41]\n","          dolphincan[3,e] = np.mean(outputs.cpu().detach().numpy(),axis=0,keepdims=True)[0,42]\n","          dolphincan[4,e] = np.mean(outputs.cpu().detach().numpy(),axis=0,keepdims=True)[0,43]\n","          dolphincan[5,e] = np.mean(outputs.cpu().detach().numpy(),axis=0,keepdims=True)[0,44]\n","        if idx in [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]:\n","          data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n","          data, target = data.cuda(), target.cuda()\n","          outputs, hidden, rep = mynet(data, target[:,:4])\n","          animals[idx,e,:] = np.mean(rep.cpu().detach().numpy(),axis=0,keepdims=True)\n","        if idx in [15,16,17,18,19,20,21,22,23,24]:\n","          data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n","          data, target = data.cuda(), target.cuda()\n","          outputs, hidden, rep = mynet(data, target[:,:4])\n","          plants[idx-15,e,:] = np.mean(rep.cpu().detach().numpy(),axis=0,keepdims=True)\n","        if idx in [3,4]:\n","          data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n","          data, target = data.cuda(), target.cuda()\n","          outputs, hidden, rep = mynet(data, target[:,:4])\n","          fish[idx-3,e,:] = np.mean(rep.cpu().detach().numpy(),axis=0,keepdims=True)\n","        if idx in [2,5]:\n","          data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n","          data, target = data.cuda(), target.cuda()\n","          outputs, hidden, rep = mynet(data, target[:,:4])\n","          if idx == 2:\n","            fish2[idx-2,e,:] = np.mean(rep.cpu().detach().numpy(),axis=0,keepdims=True)\n","          if idx == 5:\n","            fish2[idx-4,e,:] = np.mean(rep.cpu().detach().numpy(),axis=0,keepdims=True)  \n","      print('epoch ' + str(epoch_count+e) + ' loss ' + str(round(error_epoch,3)))\n","      print('time taken for the epoch ' + str(round((time.time()-t0),3)))\n","    return epoch_count + nepochs_additional, dolphincan, plants, animals, fish, fish2\n","\n","learning_rate = 0.1\n","criterion = nn.MSELoss() # mean squared error loss function\n","mynet = AlexNet(num_hidden = 512).cuda()\n","optimizer = torch.optim.SGD(mynet.parameters(), lr=learning_rate) # stochastic gradient descent\n","\n","nepochs_phase1 = 500\n","nepochs_phase2 = 1000\n","nepochs_phase3 = 1500\n","#nepochs_phase4 = 200\n","#nepochs_phase5 = 250\n","\n","epoch_count = 0\n","epoch_count, beavercan, plants, animals, fish, fish2 = train(mynet,epoch_count,nepochs_additional=nepochs_phase1)\n","#rep1 = get_rep(mynet)\n","#epoch_count = train(mynet,epoch_count,nepochs_additional=nepochs_phase2-nepochs_phase1)\n","#rep2 = get_rep(mynet)\n","#epoch_count = train(mynet,epoch_count,nepochs_additional=nepochs_phase3-nepochs_phase2)\n","#rep3 = get_rep(mynet)\n","#epoch_count = train(mynet,epoch_count,nepochs_additional=nepochs_phase4-nepochs_phase3)\n","#rep4 = get_rep(mynet)\n","#epoch_count = train(mynet,epoch_count,nepochs_additional=nepochs_phase5-nepochs_phase4)\n","#rep5 = get_rep(mynet)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 0 loss 39.572\n","time taken for the epoch 1.058\n","epoch 1 loss 34.591\n","time taken for the epoch 1.059\n","epoch 2 loss 21.843\n","time taken for the epoch 1.089\n","epoch 3 loss 7.221\n","time taken for the epoch 1.003\n","epoch 4 loss 7.142\n","time taken for the epoch 1.004\n","epoch 5 loss 7.108\n","time taken for the epoch 1.055\n","epoch 6 loss 7.087\n","time taken for the epoch 1.03\n","epoch 7 loss 7.07\n","time taken for the epoch 1.061\n","epoch 8 loss 7.055\n","time taken for the epoch 1.013\n","epoch 9 loss 7.04\n","time taken for the epoch 0.939\n","epoch 10 loss 7.025\n","time taken for the epoch 0.986\n","epoch 11 loss 7.012\n","time taken for the epoch 1.013\n","epoch 12 loss 6.997\n","time taken for the epoch 1.007\n","epoch 13 loss 6.981\n","time taken for the epoch 1.117\n","epoch 14 loss 6.967\n","time taken for the epoch 0.989\n","epoch 15 loss 6.951\n","time taken for the epoch 1.049\n","epoch 16 loss 6.934\n","time taken for the epoch 1.032\n","epoch 17 loss 6.918\n","time taken for the epoch 1.05\n","epoch 18 loss 6.901\n","time taken for the epoch 0.976\n","epoch 19 loss 6.882\n","time taken for the epoch 0.973\n","epoch 20 loss 6.862\n","time taken for the epoch 1.017\n","epoch 21 loss 6.842\n","time taken for the epoch 1.019\n","epoch 22 loss 6.821\n","time taken for the epoch 1.084\n","epoch 23 loss 6.8\n","time taken for the epoch 1.042\n","epoch 24 loss 6.777\n","time taken for the epoch 1.121\n","epoch 25 loss 6.753\n","time taken for the epoch 1.038\n","epoch 26 loss 6.728\n","time taken for the epoch 1.02\n","epoch 27 loss 6.703\n","time taken for the epoch 1.037\n","epoch 28 loss 6.676\n","time taken for the epoch 1.043\n","epoch 29 loss 6.649\n","time taken for the epoch 1.104\n","epoch 30 loss 6.62\n","time taken for the epoch 0.959\n","epoch 31 loss 6.59\n","time taken for the epoch 1.005\n","epoch 32 loss 6.56\n","time taken for the epoch 1.022\n","epoch 33 loss 6.529\n","time taken for the epoch 1.102\n","epoch 34 loss 6.497\n","time taken for the epoch 1.0\n","epoch 35 loss 6.465\n","time taken for the epoch 0.986\n","epoch 36 loss 6.431\n","time taken for the epoch 1.038\n","epoch 37 loss 6.397\n","time taken for the epoch 1.012\n","epoch 38 loss 6.363\n","time taken for the epoch 0.978\n","epoch 39 loss 6.328\n","time taken for the epoch 1.043\n","epoch 40 loss 6.293\n","time taken for the epoch 1.047\n","epoch 41 loss 6.258\n","time taken for the epoch 1.092\n","epoch 42 loss 6.223\n","time taken for the epoch 1.052\n","epoch 43 loss 6.188\n","time taken for the epoch 1.051\n","epoch 44 loss 6.152\n","time taken for the epoch 1.032\n","epoch 45 loss 6.116\n","time taken for the epoch 1.033\n","epoch 46 loss 6.081\n","time taken for the epoch 1.01\n","epoch 47 loss 6.047\n","time taken for the epoch 1.068\n","epoch 48 loss 6.013\n","time taken for the epoch 0.922\n","epoch 49 loss 5.979\n","time taken for the epoch 1.081\n","epoch 50 loss 5.946\n","time taken for the epoch 1.022\n","epoch 51 loss 5.912\n","time taken for the epoch 1.061\n","epoch 52 loss 5.881\n","time taken for the epoch 1.009\n","epoch 53 loss 5.849\n","time taken for the epoch 0.983\n","epoch 54 loss 5.817\n","time taken for the epoch 0.975\n","epoch 55 loss 5.787\n","time taken for the epoch 0.975\n","epoch 56 loss 5.756\n","time taken for the epoch 1.039\n","epoch 57 loss 5.727\n","time taken for the epoch 0.987\n","epoch 58 loss 5.698\n","time taken for the epoch 1.133\n","epoch 59 loss 5.67\n","time taken for the epoch 1.068\n","epoch 60 loss 5.643\n","time taken for the epoch 1.163\n","epoch 61 loss 5.616\n","time taken for the epoch 1.074\n","epoch 62 loss 5.59\n","time taken for the epoch 1.064\n","epoch 63 loss 5.563\n","time taken for the epoch 1.093\n","epoch 64 loss 5.538\n","time taken for the epoch 1.051\n","epoch 65 loss 5.512\n","time taken for the epoch 1.068\n","epoch 66 loss 5.487\n","time taken for the epoch 1.032\n","epoch 67 loss 5.464\n","time taken for the epoch 1.097\n","epoch 68 loss 5.441\n","time taken for the epoch 1.084\n","epoch 69 loss 5.418\n","time taken for the epoch 1.013\n","epoch 70 loss 5.396\n","time taken for the epoch 1.017\n","epoch 71 loss 5.373\n","time taken for the epoch 1.074\n","epoch 72 loss 5.351\n","time taken for the epoch 1.037\n","epoch 73 loss 5.329\n","time taken for the epoch 1.037\n","epoch 74 loss 5.309\n","time taken for the epoch 0.968\n","epoch 75 loss 5.288\n","time taken for the epoch 1.091\n","epoch 76 loss 5.268\n","time taken for the epoch 1.029\n","epoch 77 loss 5.248\n","time taken for the epoch 1.104\n","epoch 78 loss 5.229\n","time taken for the epoch 0.972\n","epoch 79 loss 5.209\n","time taken for the epoch 0.971\n","epoch 80 loss 5.19\n","time taken for the epoch 1.068\n","epoch 81 loss 5.172\n","time taken for the epoch 1.154\n","epoch 82 loss 5.153\n","time taken for the epoch 1.096\n","epoch 83 loss 5.136\n","time taken for the epoch 0.97\n","epoch 84 loss 5.118\n","time taken for the epoch 1.021\n","epoch 85 loss 5.101\n","time taken for the epoch 1.012\n","epoch 86 loss 5.083\n","time taken for the epoch 1.042\n","epoch 87 loss 5.067\n","time taken for the epoch 1.029\n","epoch 88 loss 5.05\n","time taken for the epoch 1.054\n","epoch 89 loss 5.035\n","time taken for the epoch 1.007\n","epoch 90 loss 5.019\n","time taken for the epoch 0.98\n","epoch 91 loss 5.003\n","time taken for the epoch 1.039\n","epoch 92 loss 4.989\n","time taken for the epoch 0.949\n","epoch 93 loss 4.974\n","time taken for the epoch 0.976\n","epoch 94 loss 4.958\n","time taken for the epoch 0.995\n","epoch 95 loss 4.945\n","time taken for the epoch 1.167\n","epoch 96 loss 4.929\n","time taken for the epoch 1.1\n","epoch 97 loss 4.915\n","time taken for the epoch 1.045\n","epoch 98 loss 4.902\n","time taken for the epoch 0.981\n","epoch 99 loss 4.889\n","time taken for the epoch 1.067\n","epoch 100 loss 4.876\n","time taken for the epoch 0.994\n","epoch 101 loss 4.863\n","time taken for the epoch 1.035\n","epoch 102 loss 4.85\n","time taken for the epoch 0.987\n","epoch 103 loss 4.838\n","time taken for the epoch 1.069\n","epoch 104 loss 4.826\n","time taken for the epoch 1.044\n","epoch 105 loss 4.814\n","time taken for the epoch 1.028\n","epoch 106 loss 4.802\n","time taken for the epoch 1.041\n","epoch 107 loss 4.791\n","time taken for the epoch 1.051\n","epoch 108 loss 4.779\n","time taken for the epoch 1.072\n","epoch 109 loss 4.769\n","time taken for the epoch 1.109\n","epoch 110 loss 4.758\n","time taken for the epoch 1.127\n","epoch 111 loss 4.747\n","time taken for the epoch 1.031\n","epoch 112 loss 4.737\n","time taken for the epoch 0.992\n","epoch 113 loss 4.727\n","time taken for the epoch 1.083\n","epoch 114 loss 4.718\n","time taken for the epoch 1.033\n","epoch 115 loss 4.708\n","time taken for the epoch 1.139\n","epoch 116 loss 4.699\n","time taken for the epoch 1.025\n","epoch 117 loss 4.69\n","time taken for the epoch 1.076\n","epoch 118 loss 4.681\n","time taken for the epoch 1.073\n","epoch 119 loss 4.672\n","time taken for the epoch 1.14\n","epoch 120 loss 4.664\n","time taken for the epoch 1.125\n","epoch 121 loss 4.657\n","time taken for the epoch 1.016\n","epoch 122 loss 4.649\n","time taken for the epoch 1.037\n","epoch 123 loss 4.641\n","time taken for the epoch 1.132\n","epoch 124 loss 4.634\n","time taken for the epoch 0.987\n","epoch 125 loss 4.627\n","time taken for the epoch 1.003\n","epoch 126 loss 4.619\n","time taken for the epoch 1.0\n","epoch 127 loss 4.613\n","time taken for the epoch 0.961\n","epoch 128 loss 4.606\n","time taken for the epoch 1.049\n","epoch 129 loss 4.6\n","time taken for the epoch 1.129\n","epoch 130 loss 4.593\n","time taken for the epoch 1.129\n","epoch 131 loss 4.588\n","time taken for the epoch 1.064\n","epoch 132 loss 4.581\n","time taken for the epoch 1.088\n","epoch 133 loss 4.576\n","time taken for the epoch 0.994\n","epoch 134 loss 4.571\n","time taken for the epoch 0.964\n","epoch 135 loss 4.564\n","time taken for the epoch 1.068\n","epoch 136 loss 4.559\n","time taken for the epoch 1.001\n","epoch 137 loss 4.555\n","time taken for the epoch 1.058\n","epoch 138 loss 4.55\n","time taken for the epoch 1.027\n","epoch 139 loss 4.544\n","time taken for the epoch 1.174\n","epoch 140 loss 4.539\n","time taken for the epoch 1.085\n","epoch 141 loss 4.535\n","time taken for the epoch 1.031\n","epoch 142 loss 4.531\n","time taken for the epoch 1.069\n","epoch 143 loss 4.527\n","time taken for the epoch 1.072\n","epoch 144 loss 4.521\n","time taken for the epoch 1.016\n","epoch 145 loss 4.517\n","time taken for the epoch 1.022\n","epoch 146 loss 4.512\n","time taken for the epoch 1.067\n","epoch 147 loss 4.508\n","time taken for the epoch 1.02\n","epoch 148 loss 4.505\n","time taken for the epoch 1.081\n","epoch 149 loss 4.5\n","time taken for the epoch 1.089\n","epoch 150 loss 4.497\n","time taken for the epoch 1.091\n","epoch 151 loss 4.493\n","time taken for the epoch 0.939\n","epoch 152 loss 4.489\n","time taken for the epoch 1.063\n","epoch 153 loss 4.485\n","time taken for the epoch 1.016\n","epoch 154 loss 4.482\n","time taken for the epoch 1.048\n","epoch 155 loss 4.479\n","time taken for the epoch 1.011\n","epoch 156 loss 4.474\n","time taken for the epoch 0.976\n","epoch 157 loss 4.471\n","time taken for the epoch 1.036\n","epoch 158 loss 4.467\n","time taken for the epoch 1.034\n","epoch 159 loss 4.463\n","time taken for the epoch 1.103\n","epoch 160 loss 4.461\n","time taken for the epoch 1.023\n","epoch 161 loss 4.457\n","time taken for the epoch 0.983\n","epoch 162 loss 4.454\n","time taken for the epoch 1.034\n","epoch 163 loss 4.45\n","time taken for the epoch 0.98\n","epoch 164 loss 4.448\n","time taken for the epoch 1.047\n","epoch 165 loss 4.445\n","time taken for the epoch 1.068\n","epoch 166 loss 4.441\n","time taken for the epoch 1.11\n","epoch 167 loss 4.437\n","time taken for the epoch 1.015\n","epoch 168 loss 4.434\n","time taken for the epoch 0.957\n","epoch 169 loss 4.432\n","time taken for the epoch 1.002\n","epoch 170 loss 4.429\n","time taken for the epoch 0.952\n","epoch 171 loss 4.426\n","time taken for the epoch 1.092\n","epoch 172 loss 4.423\n","time taken for the epoch 1.04\n","epoch 173 loss 4.42\n","time taken for the epoch 1.046\n","epoch 174 loss 4.417\n","time taken for the epoch 1.04\n","epoch 175 loss 4.414\n","time taken for the epoch 1.048\n","epoch 176 loss 4.411\n","time taken for the epoch 1.027\n","epoch 177 loss 4.408\n","time taken for the epoch 1.036\n","epoch 178 loss 4.404\n","time taken for the epoch 0.918\n","epoch 179 loss 4.403\n","time taken for the epoch 1.07\n","epoch 180 loss 4.399\n","time taken for the epoch 0.904\n","epoch 181 loss 4.396\n","time taken for the epoch 1.036\n","epoch 182 loss 4.394\n","time taken for the epoch 1.068\n","epoch 183 loss 4.391\n","time taken for the epoch 1.109\n","epoch 184 loss 4.388\n","time taken for the epoch 1.008\n","epoch 185 loss 4.386\n","time taken for the epoch 0.937\n","epoch 186 loss 4.383\n","time taken for the epoch 1.117\n","epoch 187 loss 4.38\n","time taken for the epoch 1.027\n","epoch 188 loss 4.377\n","time taken for the epoch 0.977\n","epoch 189 loss 4.374\n","time taken for the epoch 0.986\n","epoch 190 loss 4.371\n","time taken for the epoch 1.002\n","epoch 191 loss 4.368\n","time taken for the epoch 0.961\n","epoch 192 loss 4.365\n","time taken for the epoch 1.101\n","epoch 193 loss 4.363\n","time taken for the epoch 1.034\n","epoch 194 loss 4.359\n","time taken for the epoch 1.041\n","epoch 195 loss 4.356\n","time taken for the epoch 0.94\n","epoch 196 loss 4.355\n","time taken for the epoch 1.039\n","epoch 197 loss 4.351\n","time taken for the epoch 1.092\n","epoch 198 loss 4.348\n","time taken for the epoch 0.942\n","epoch 199 loss 4.345\n","time taken for the epoch 0.938\n","epoch 200 loss 4.342\n","time taken for the epoch 1.057\n","epoch 201 loss 4.339\n","time taken for the epoch 1.054\n","epoch 202 loss 4.336\n","time taken for the epoch 0.994\n","epoch 203 loss 4.333\n","time taken for the epoch 1.027\n","epoch 204 loss 4.33\n","time taken for the epoch 1.069\n","epoch 205 loss 4.327\n","time taken for the epoch 1.038\n","epoch 206 loss 4.323\n","time taken for the epoch 1.011\n","epoch 207 loss 4.321\n","time taken for the epoch 1.005\n","epoch 208 loss 4.318\n","time taken for the epoch 1.111\n","epoch 209 loss 4.314\n","time taken for the epoch 1.136\n","epoch 210 loss 4.31\n","time taken for the epoch 1.126\n","epoch 211 loss 4.307\n","time taken for the epoch 0.971\n","epoch 212 loss 4.303\n","time taken for the epoch 1.011\n","epoch 213 loss 4.3\n","time taken for the epoch 1.077\n","epoch 214 loss 4.296\n","time taken for the epoch 1.05\n","epoch 215 loss 4.292\n","time taken for the epoch 1.042\n","epoch 216 loss 4.287\n","time taken for the epoch 0.998\n","epoch 217 loss 4.284\n","time taken for the epoch 1.152\n","epoch 218 loss 4.28\n","time taken for the epoch 0.999\n","epoch 219 loss 4.276\n","time taken for the epoch 1.046\n","epoch 220 loss 4.272\n","time taken for the epoch 0.975\n","epoch 221 loss 4.267\n","time taken for the epoch 1.045\n","epoch 222 loss 4.262\n","time taken for the epoch 1.054\n","epoch 223 loss 4.258\n","time taken for the epoch 0.998\n","epoch 224 loss 4.252\n","time taken for the epoch 1.012\n","epoch 225 loss 4.247\n","time taken for the epoch 1.056\n","epoch 226 loss 4.241\n","time taken for the epoch 1.057\n","epoch 227 loss 4.235\n","time taken for the epoch 1.142\n","epoch 228 loss 4.228\n","time taken for the epoch 0.964\n","epoch 229 loss 4.223\n","time taken for the epoch 0.992\n","epoch 230 loss 4.215\n","time taken for the epoch 1.029\n","epoch 231 loss 4.209\n","time taken for the epoch 1.065\n","epoch 232 loss 4.2\n","time taken for the epoch 1.008\n","epoch 233 loss 4.192\n","time taken for the epoch 1.043\n","epoch 234 loss 4.183\n","time taken for the epoch 1.062\n","epoch 235 loss 4.174\n","time taken for the epoch 1.001\n","epoch 236 loss 4.164\n","time taken for the epoch 1.098\n","epoch 237 loss 4.154\n","time taken for the epoch 1.028\n","epoch 238 loss 4.142\n","time taken for the epoch 0.999\n","epoch 239 loss 4.132\n","time taken for the epoch 1.072\n","epoch 240 loss 4.119\n","time taken for the epoch 1.049\n","epoch 241 loss 4.107\n","time taken for the epoch 1.095\n","epoch 242 loss 4.093\n","time taken for the epoch 1.07\n","epoch 243 loss 4.08\n","time taken for the epoch 1.048\n","epoch 244 loss 4.067\n","time taken for the epoch 1.057\n","epoch 245 loss 4.054\n","time taken for the epoch 1.098\n","epoch 246 loss 4.039\n","time taken for the epoch 0.99\n","epoch 247 loss 4.025\n","time taken for the epoch 1.019\n","epoch 248 loss 4.012\n","time taken for the epoch 1.034\n","epoch 249 loss 3.998\n","time taken for the epoch 1.03\n","epoch 250 loss 3.984\n","time taken for the epoch 1.007\n","epoch 251 loss 3.969\n","time taken for the epoch 1.181\n","epoch 252 loss 3.956\n","time taken for the epoch 1.072\n","epoch 253 loss 3.944\n","time taken for the epoch 1.114\n","epoch 254 loss 3.93\n","time taken for the epoch 1.092\n","epoch 255 loss 3.917\n","time taken for the epoch 1.04\n","epoch 256 loss 3.903\n","time taken for the epoch 1.081\n","epoch 257 loss 3.891\n","time taken for the epoch 0.972\n","epoch 258 loss 3.878\n","time taken for the epoch 0.965\n","epoch 259 loss 3.866\n","time taken for the epoch 0.994\n","epoch 260 loss 3.853\n","time taken for the epoch 0.988\n","epoch 261 loss 3.841\n","time taken for the epoch 1.002\n","epoch 262 loss 3.83\n","time taken for the epoch 1.072\n","epoch 263 loss 3.818\n","time taken for the epoch 1.015\n","epoch 264 loss 3.807\n","time taken for the epoch 1.028\n","epoch 265 loss 3.796\n","time taken for the epoch 1.016\n","epoch 266 loss 3.785\n","time taken for the epoch 1.003\n","epoch 267 loss 3.774\n","time taken for the epoch 1.061\n","epoch 268 loss 3.764\n","time taken for the epoch 0.914\n","epoch 269 loss 3.753\n","time taken for the epoch 0.956\n","epoch 270 loss 3.743\n","time taken for the epoch 1.025\n","epoch 271 loss 3.734\n","time taken for the epoch 1.067\n","epoch 272 loss 3.724\n","time taken for the epoch 1.035\n","epoch 273 loss 3.714\n","time taken for the epoch 1.032\n","epoch 274 loss 3.706\n","time taken for the epoch 1.004\n","epoch 275 loss 3.696\n","time taken for the epoch 1.035\n","epoch 276 loss 3.687\n","time taken for the epoch 0.97\n","epoch 277 loss 3.678\n","time taken for the epoch 0.998\n","epoch 278 loss 3.67\n","time taken for the epoch 0.893\n","epoch 279 loss 3.661\n","time taken for the epoch 1.053\n","epoch 280 loss 3.652\n","time taken for the epoch 1.084\n","epoch 281 loss 3.644\n","time taken for the epoch 1.011\n","epoch 282 loss 3.635\n","time taken for the epoch 0.949\n","epoch 283 loss 3.627\n","time taken for the epoch 1.076\n","epoch 284 loss 3.621\n","time taken for the epoch 0.989\n","epoch 285 loss 3.613\n","time taken for the epoch 1.122\n","epoch 286 loss 3.604\n","time taken for the epoch 1.212\n","epoch 287 loss 3.597\n","time taken for the epoch 1.016\n","epoch 288 loss 3.591\n","time taken for the epoch 0.988\n","epoch 289 loss 3.584\n","time taken for the epoch 1.062\n","epoch 290 loss 3.576\n","time taken for the epoch 0.997\n","epoch 291 loss 3.569\n","time taken for the epoch 0.992\n","epoch 292 loss 3.562\n","time taken for the epoch 1.033\n","epoch 293 loss 3.556\n","time taken for the epoch 1.029\n","epoch 294 loss 3.549\n","time taken for the epoch 0.992\n","epoch 295 loss 3.542\n","time taken for the epoch 1.011\n","epoch 296 loss 3.538\n","time taken for the epoch 0.94\n","epoch 297 loss 3.531\n","time taken for the epoch 1.144\n","epoch 298 loss 3.525\n","time taken for the epoch 1.07\n","epoch 299 loss 3.519\n","time taken for the epoch 0.971\n","epoch 300 loss 3.512\n","time taken for the epoch 0.986\n","epoch 301 loss 3.509\n","time taken for the epoch 0.952\n","epoch 302 loss 3.5\n","time taken for the epoch 0.969\n","epoch 303 loss 3.497\n","time taken for the epoch 0.97\n","epoch 304 loss 3.489\n","time taken for the epoch 1.03\n","epoch 305 loss 3.484\n","time taken for the epoch 0.959\n","epoch 306 loss 3.478\n","time taken for the epoch 1.045\n","epoch 307 loss 3.472\n","time taken for the epoch 0.985\n","epoch 308 loss 3.468\n","time taken for the epoch 1.045\n","epoch 309 loss 3.462\n","time taken for the epoch 1.0\n","epoch 310 loss 3.456\n","time taken for the epoch 1.081\n","epoch 311 loss 3.451\n","time taken for the epoch 1.083\n","epoch 312 loss 3.446\n","time taken for the epoch 0.951\n","epoch 313 loss 3.44\n","time taken for the epoch 0.993\n","epoch 314 loss 3.436\n","time taken for the epoch 1.04\n","epoch 315 loss 3.431\n","time taken for the epoch 1.078\n","epoch 316 loss 3.424\n","time taken for the epoch 0.99\n","epoch 317 loss 3.42\n","time taken for the epoch 1.0\n","epoch 318 loss 3.414\n","time taken for the epoch 1.033\n","epoch 319 loss 3.41\n","time taken for the epoch 1.117\n","epoch 320 loss 3.406\n","time taken for the epoch 1.025\n","epoch 321 loss 3.4\n","time taken for the epoch 1.012\n","epoch 322 loss 3.394\n","time taken for the epoch 0.952\n","epoch 323 loss 3.39\n","time taken for the epoch 1.044\n","epoch 324 loss 3.386\n","time taken for the epoch 1.07\n","epoch 325 loss 3.38\n","time taken for the epoch 1.055\n","epoch 326 loss 3.376\n","time taken for the epoch 1.019\n","epoch 327 loss 3.368\n","time taken for the epoch 0.962\n","epoch 328 loss 3.368\n","time taken for the epoch 0.998\n","epoch 329 loss 3.36\n","time taken for the epoch 1.049\n","epoch 330 loss 3.357\n","time taken for the epoch 0.982\n","epoch 331 loss 3.352\n","time taken for the epoch 1.037\n","epoch 332 loss 3.346\n","time taken for the epoch 1.036\n","epoch 333 loss 3.341\n","time taken for the epoch 1.11\n","epoch 334 loss 3.34\n","time taken for the epoch 0.956\n","epoch 335 loss 3.332\n","time taken for the epoch 0.992\n","epoch 336 loss 3.331\n","time taken for the epoch 0.991\n","epoch 337 loss 3.321\n","time taken for the epoch 1.043\n","epoch 338 loss 3.316\n","time taken for the epoch 0.909\n","epoch 339 loss 3.316\n","time taken for the epoch 0.971\n","epoch 340 loss 3.31\n","time taken for the epoch 0.952\n","epoch 341 loss 3.302\n","time taken for the epoch 1.02\n","epoch 342 loss 3.296\n","time taken for the epoch 1.092\n","epoch 343 loss 3.3\n","time taken for the epoch 1.006\n","epoch 344 loss 3.291\n","time taken for the epoch 0.959\n","epoch 345 loss 3.287\n","time taken for the epoch 1.036\n","epoch 346 loss 3.286\n","time taken for the epoch 1.006\n","epoch 347 loss 3.278\n","time taken for the epoch 0.982\n","epoch 348 loss 3.273\n","time taken for the epoch 1.0\n","epoch 349 loss 3.27\n","time taken for the epoch 0.999\n","epoch 350 loss 3.265\n","time taken for the epoch 1.052\n","epoch 351 loss 3.257\n","time taken for the epoch 0.968\n","epoch 352 loss 3.259\n","time taken for the epoch 1.042\n","epoch 353 loss 3.249\n","time taken for the epoch 1.057\n","epoch 354 loss 3.243\n","time taken for the epoch 1.088\n","epoch 355 loss 3.242\n","time taken for the epoch 1.001\n","epoch 356 loss 3.238\n","time taken for the epoch 0.982\n","epoch 357 loss 3.239\n","time taken for the epoch 1.034\n","epoch 358 loss 3.233\n","time taken for the epoch 0.969\n","epoch 359 loss 3.232\n","time taken for the epoch 1.136\n","epoch 360 loss 3.226\n","time taken for the epoch 1.05\n","epoch 361 loss 3.215\n","time taken for the epoch 1.021\n","epoch 362 loss 3.218\n","time taken for the epoch 1.025\n","epoch 363 loss 3.213\n","time taken for the epoch 0.996\n","epoch 364 loss 3.208\n","time taken for the epoch 1.055\n","epoch 365 loss 3.199\n","time taken for the epoch 0.967\n","epoch 366 loss 3.194\n","time taken for the epoch 1.017\n","epoch 367 loss 3.194\n","time taken for the epoch 0.988\n","epoch 368 loss 3.191\n","time taken for the epoch 1.178\n","epoch 369 loss 3.192\n","time taken for the epoch 1.001\n","epoch 370 loss 3.184\n","time taken for the epoch 1.046\n","epoch 371 loss 3.181\n","time taken for the epoch 0.973\n","epoch 372 loss 3.175\n","time taken for the epoch 1.036\n","epoch 373 loss 3.181\n","time taken for the epoch 0.977\n","epoch 374 loss 3.167\n","time taken for the epoch 0.997\n","epoch 375 loss 3.159\n","time taken for the epoch 1.016\n","epoch 376 loss 3.161\n","time taken for the epoch 1.019\n","epoch 377 loss 3.153\n","time taken for the epoch 1.063\n","epoch 378 loss 3.155\n","time taken for the epoch 1.033\n","epoch 379 loss 3.159\n","time taken for the epoch 1.039\n","epoch 380 loss 3.141\n","time taken for the epoch 1.032\n","epoch 381 loss 3.141\n","time taken for the epoch 1.0\n","epoch 382 loss 3.139\n","time taken for the epoch 0.999\n","epoch 383 loss 3.139\n","time taken for the epoch 0.998\n","epoch 384 loss 3.123\n","time taken for the epoch 1.045\n","epoch 385 loss 3.132\n","time taken for the epoch 1.088\n","epoch 386 loss 3.134\n","time taken for the epoch 1.074\n","epoch 387 loss 3.116\n","time taken for the epoch 1.129\n","epoch 388 loss 3.108\n","time taken for the epoch 1.008\n","epoch 389 loss 3.117\n","time taken for the epoch 0.97\n","epoch 390 loss 3.096\n","time taken for the epoch 1.064\n","epoch 391 loss 3.099\n","time taken for the epoch 1.044\n","epoch 392 loss 3.088\n","time taken for the epoch 0.987\n","epoch 393 loss 3.095\n","time taken for the epoch 1.02\n","epoch 394 loss 3.088\n","time taken for the epoch 1.094\n","epoch 395 loss 3.09\n","time taken for the epoch 1.008\n","epoch 396 loss 3.083\n","time taken for the epoch 0.951\n","epoch 397 loss 3.079\n","time taken for the epoch 1.033\n","epoch 398 loss 3.074\n","time taken for the epoch 1.052\n","epoch 399 loss 3.073\n","time taken for the epoch 1.039\n","epoch 400 loss 3.065\n","time taken for the epoch 1.06\n","epoch 401 loss 3.067\n","time taken for the epoch 1.003\n","epoch 402 loss 3.049\n","time taken for the epoch 1.068\n","epoch 403 loss 3.051\n","time taken for the epoch 1.099\n","epoch 404 loss 3.044\n","time taken for the epoch 1.046\n","epoch 405 loss 3.044\n","time taken for the epoch 1.024\n","epoch 406 loss 3.04\n","time taken for the epoch 1.067\n","epoch 407 loss 3.03\n","time taken for the epoch 0.961\n","epoch 408 loss 3.033\n","time taken for the epoch 1.024\n","epoch 409 loss 3.032\n","time taken for the epoch 1.026\n","epoch 410 loss 3.025\n","time taken for the epoch 0.932\n","epoch 411 loss 3.011\n","time taken for the epoch 1.042\n","epoch 412 loss 3.02\n","time taken for the epoch 1.178\n","epoch 413 loss 3.007\n","time taken for the epoch 1.009\n","epoch 414 loss 2.998\n","time taken for the epoch 1.108\n","epoch 415 loss 3.013\n","time taken for the epoch 0.933\n","epoch 416 loss 2.995\n","time taken for the epoch 1.004\n","epoch 417 loss 2.991\n","time taken for the epoch 0.981\n","epoch 418 loss 2.978\n","time taken for the epoch 1.081\n","epoch 419 loss 2.976\n","time taken for the epoch 1.041\n","epoch 420 loss 2.987\n","time taken for the epoch 1.164\n","epoch 421 loss 2.972\n","time taken for the epoch 1.202\n","epoch 422 loss 2.964\n","time taken for the epoch 0.951\n","epoch 423 loss 2.96\n","time taken for the epoch 0.992\n","epoch 424 loss 2.956\n","time taken for the epoch 1.091\n","epoch 425 loss 2.946\n","time taken for the epoch 1.007\n","epoch 426 loss 2.945\n","time taken for the epoch 1.082\n","epoch 427 loss 2.943\n","time taken for the epoch 0.987\n","epoch 428 loss 2.937\n","time taken for the epoch 1.105\n","epoch 429 loss 2.921\n","time taken for the epoch 1.094\n","epoch 430 loss 2.918\n","time taken for the epoch 0.997\n","epoch 431 loss 2.907\n","time taken for the epoch 0.99\n","epoch 432 loss 2.916\n","time taken for the epoch 1.039\n","epoch 433 loss 2.905\n","time taken for the epoch 1.05\n","epoch 434 loss 2.9\n","time taken for the epoch 1.065\n","epoch 435 loss 2.88\n","time taken for the epoch 1.114\n","epoch 436 loss 2.885\n","time taken for the epoch 1.007\n","epoch 437 loss 2.875\n","time taken for the epoch 0.98\n","epoch 438 loss 2.871\n","time taken for the epoch 1.033\n","epoch 439 loss 2.877\n","time taken for the epoch 1.139\n","epoch 440 loss 2.857\n","time taken for the epoch 1.112\n","epoch 441 loss 2.847\n","time taken for the epoch 1.014\n","epoch 442 loss 2.86\n","time taken for the epoch 1.027\n","epoch 443 loss 2.828\n","time taken for the epoch 1.008\n","epoch 444 loss 2.83\n","time taken for the epoch 1.04\n","epoch 445 loss 2.844\n","time taken for the epoch 1.022\n","epoch 446 loss 2.814\n","time taken for the epoch 0.969\n","epoch 447 loss 2.8\n","time taken for the epoch 1.024\n","epoch 448 loss 2.808\n","time taken for the epoch 1.04\n","epoch 449 loss 2.79\n","time taken for the epoch 1.094\n","epoch 450 loss 2.801\n","time taken for the epoch 1.03\n","epoch 451 loss 2.789\n","time taken for the epoch 1.069\n","epoch 452 loss 2.786\n","time taken for the epoch 1.032\n","epoch 453 loss 2.777\n","time taken for the epoch 1.026\n","epoch 454 loss 2.767\n","time taken for the epoch 1.073\n","epoch 455 loss 2.767\n","time taken for the epoch 1.102\n","epoch 456 loss 2.755\n","time taken for the epoch 1.094\n","epoch 457 loss 2.753\n","time taken for the epoch 1.037\n","epoch 458 loss 2.747\n","time taken for the epoch 1.049\n","epoch 459 loss 2.74\n","time taken for the epoch 1.033\n","epoch 460 loss 2.733\n","time taken for the epoch 1.08\n","epoch 461 loss 2.728\n","time taken for the epoch 1.002\n","epoch 462 loss 2.733\n","time taken for the epoch 0.999\n","epoch 463 loss 2.73\n","time taken for the epoch 0.969\n","epoch 464 loss 2.722\n","time taken for the epoch 1.161\n","epoch 465 loss 2.724\n","time taken for the epoch 0.978\n","epoch 466 loss 2.705\n","time taken for the epoch 0.955\n","epoch 467 loss 2.708\n","time taken for the epoch 1.098\n","epoch 468 loss 2.705\n","time taken for the epoch 1.011\n","epoch 469 loss 2.694\n","time taken for the epoch 0.952\n","epoch 470 loss 2.693\n","time taken for the epoch 0.992\n","epoch 471 loss 2.699\n","time taken for the epoch 0.953\n","epoch 472 loss 2.677\n","time taken for the epoch 1.01\n","epoch 473 loss 2.686\n","time taken for the epoch 1.161\n","epoch 474 loss 2.682\n","time taken for the epoch 1.131\n","epoch 475 loss 2.673\n","time taken for the epoch 1.028\n","epoch 476 loss 2.658\n","time taken for the epoch 1.024\n","epoch 477 loss 2.659\n","time taken for the epoch 0.998\n","epoch 478 loss 2.669\n","time taken for the epoch 1.016\n","epoch 479 loss 2.654\n","time taken for the epoch 0.999\n","epoch 480 loss 2.651\n","time taken for the epoch 1.026\n","epoch 481 loss 2.637\n","time taken for the epoch 1.081\n","epoch 482 loss 2.639\n","time taken for the epoch 1.214\n","epoch 483 loss 2.652\n","time taken for the epoch 1.098\n","epoch 484 loss 2.64\n","time taken for the epoch 1.005\n","epoch 485 loss 2.636\n","time taken for the epoch 1.025\n","epoch 486 loss 2.628\n","time taken for the epoch 1.115\n","epoch 487 loss 2.622\n","time taken for the epoch 0.986\n","epoch 488 loss 2.624\n","time taken for the epoch 1.135\n","epoch 489 loss 2.615\n","time taken for the epoch 1.027\n","epoch 490 loss 2.613\n","time taken for the epoch 1.19\n","epoch 491 loss 2.602\n","time taken for the epoch 1.004\n","epoch 492 loss 2.6\n","time taken for the epoch 1.018\n","epoch 493 loss 2.605\n","time taken for the epoch 0.977\n","epoch 494 loss 2.592\n","time taken for the epoch 1.079\n","epoch 495 loss 2.598\n","time taken for the epoch 1.051\n","epoch 496 loss 2.587\n","time taken for the epoch 1.027\n","epoch 497 loss 2.583\n","time taken for the epoch 1.114\n","epoch 498 loss 2.577\n","time taken for the epoch 1.031\n","epoch 499 loss 2.578\n","time taken for the epoch 1.071\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-fb84f2624bde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mepoch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeavercan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmynet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnepochs_additional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnepochs_phase1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;31m#rep1 = get_rep(mynet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m#epoch_count = train(mynet,epoch_count,nepochs_additional=nepochs_phase2-nepochs_phase1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"]}]},{"metadata":{"id":"VZWkD99T7Snh","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.plot(beavercan[0,:],'b')\n","plt.plot(beavercan[1,:],'g')\n","plt.plot(beavercan[2,:],'r')\n","plt.plot(beavercan[3,:],'c')\n","plt.plot(beavercan[4,:],'m')\n","plt.plot(beavercan[5,:],'y')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zXEw60bt7igz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":171},"outputId":"fa98786b-924f-47a2-85ea-cfd1676cbf70","executionInfo":{"status":"error","timestamp":1556576707705,"user_tz":240,"elapsed":306,"user":{"displayName":"Arihant Jain","photoUrl":"https://lh5.googleusercontent.com/-3kii2uSyBNA/AAAAAAAAAAI/AAAAAAAAHwU/T3ZvQABdreo/s64/photo.jpg","userId":"03591714780569092811"}}},"cell_type":"code","source":[""],"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-583dc9d98604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfish2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'fish2' is not defined"]}]},{"metadata":{"id":"JSeGIuMaM6JV","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}